import pandas as pd
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
from kiwipiepy import Kiwi
from umap import UMAP
from sentence_transformers import SentenceTransformer
import os
import time
from tqdm import tqdm
import numpy as np
import hashlib
import re

# ==================================================================================
# [ì„¤ì • ë° ìƒìˆ˜ ì •ì˜]
# ==================================================================================
INPUT_FILE = "../../data_crawl/FINAL_RESULT.csv"
OUTPUT_DIR_ROOT = "../data/topic_model_results"

# 1. ê¸°ë³¸ ë¶ˆìš©ì–´ (ê³µí†µ)
STOPWORDS_COMMON = {
    'ì œí’ˆ', 'êµ¬ë§¤', 'ì‚¬ìš©', 'ì§„ì§œ', 'ì™„ì „', 'ë„ˆë¬´', 'ì •ë§', 'ê²ƒ', 'ìˆ˜', 'ì €', 'ì´', 'ê±°', 
    'ìƒí’ˆ', 'ì£¼ë¬¸', 'ë„ì°©', 'ìƒê°', 'ì‚¬ëŒ', 'ë§ˆìŒ', 'ì¤€ë¹„', 'ê¸°ê°„', 'ì •ë„', 'ëŠë‚Œ',
    'ì˜¤ëŠ˜', 'ì´ë²ˆ', 'ì—­ì‹œ', 'í•­ìƒ', 'ë•Œë¬¸', 'ë¶€ë¶„', 'ê·¼ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ '
}

# 2. ë¸Œëœë“œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ (ê³µí†µ)
BRANDS = {
    "ì„¤í™”ìˆ˜", "ë§ˆëª½ë“œ", "ì—ìŠ¤ì˜ì•„", "ì•„ì´ì˜¤í˜", "í•´í”¼ë°”ìŠ¤", "ë¼ë„¤ì¦ˆ", "í”„ë¦¬ë©”ë¼", "í—¤ë¼", "ì¼ë¦¬ìœ¤", "í•œìœ¨", "ë¯¸ìŸì„¼", "ë ¤", "ë°”ì´íƒˆë·°í‹°",
    "sulwhasoo", "mamonde", "espoir", "iope", "happybath", "laneige", "primera", "hera", "illiyoon", "hanyul", "miseenscene", "ryo", "vitalbeauty"
}
BRANDS |= {b.lower() for b in BRANDS}

# 3. ë©”íƒ€/í‰ê°€ ë¶ˆìš©ì–´ (íš¨ëŠ¥ ëª¨ë“œìš©: êµ¬ë§¤/ê°íƒ„/ë°°ì†¡ ê´€ë ¨ ì œê±°)
STOPWORDS_META = {
    "ë°°ì†¡", "í¬ì¥", "ì‚¬ì€í’ˆ", "ì¶”ì²œ", "ìµœê³ ", "ì¬êµ¬ë§¤", "ë§Œì¡±", "ë³„ë¡œ", "ê°•ì¶”", "ë¹„ì¶”", 
    "ì„ ë¬¼", "ì¿ í°", "ê°€ê²©", "í–‰ì‚¬", "ì„¸ì¼", "ë°•ìŠ¤", "íƒë°°", "ê¸°ì‚¬", "ë„ì°©", "ë¹ ë¦„"
}

# 4. íš¨ëŠ¥ ê´€ë ¨ SL(ì˜ì–´) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (íš¨ëŠ¥ ëª¨ë“œìš©)
EFFICACY_SL_WHITELIST = {
    "SPF","PA","UVA","UVB","UV","AHA","BHA","PHA","CICA","TECA","PH",
    "RETINOL","NIACINAMIDE","CERAMIDE","HA","HYA","VITAMIN","COLLAGEN",
    "PANTHENOL","TEA","TREE","MUGWORT"
}
EFFICACY_SL_WHITELIST |= {w.lower() for w in EFFICACY_SL_WHITELIST}

# ==================================================================================
# [Helper Functions]
# ==================================================================================

def compute_data_fingerprint(docs):
    """
    ë°ì´í„° ì§€ë¬¸ ìƒì„± (ê²€ì¦ ê°•í™”: ìƒ˜í”Œ ì¦ê°€ + ì•ë’¤ ë‚´ìš© ì‚¬ìš©)
    """
    if not docs:
        return "empty"
    n = len(docs)
    # ìƒ˜í”Œ 50ê°œ ì¶”ì¶œ
    sample_indices = np.linspace(0, n - 1, num=min(50, n), dtype=int)
    
    signature_str = f"{n}"
    for idx in sample_indices:
        # ì• 50ì + ë’¤ 50ì ì‚¬ìš©í•˜ì—¬ ì¤‘ê°„ ë‚´ìš© ë³€ê²½ ê°ì§€ ê°•í™”
        d_str = str(docs[idx])
        signature_str += f"|{d_str[:50]}|{d_str[-50:]}"
        
    return hashlib.md5(signature_str.encode('utf-8')).hexdigest()

def pre_tokenize(texts, mode='EFFICACY'):
    """
    Kiwi í† í°í™” (ëª¨ë“œë³„ ì°¨ë³„í™”)
    - mode='EFFICACY': íš¨ëŠ¥/ì‚¬ìš©ê° ì¤‘ì‹¬. SL í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì ìš©, ë©”íƒ€ì–´ ì œê±°.
    - mode='PURCHASE': êµ¬ë§¤íŒ¨í„´/ë¼ì´í”„ìŠ¤íƒ€ì¼ í¬í•¨. SL í—ˆìš©í­ ë„“ìŒ.
    """
    kiwi = Kiwi(num_workers=4)
    print(f"ğŸš€ Pre-tokenizing Reviews (Mode: {mode})...")
    
    tokenized_docs = []
    
    # ì •ê·œì‹ íŒ¨í„´ (fullmatchìš© ^...$)
    # í•œê¸€ 2ê¸€ì ì´ìƒ OR ì˜ë¬¸/ìˆ«ì 2ê¸€ì ì´ìƒ
    # (íš¨ëŠ¥ ëª¨ë“œì—ì„œëŠ” SL Whitelistë¡œ 2ì°¨ ê²€ì¦í•˜ë¯€ë¡œ ì—¬ê¸°ì„  Broadí•˜ê²Œ ì¡ìŒ)
    valid_pattern = re.compile(r'^(?:[ê°€-í£]{2,}|[a-zA-Z0-9]{2,})$')
    
    # ëª¨ë“œë³„ ë¶ˆìš©ì–´ ì„¤ì •
    if mode == 'EFFICACY':
        final_stopwords = STOPWORDS_COMMON | STOPWORDS_META
    else: # PURCHASE
        final_stopwords = STOPWORDS_COMMON 
        # PURCHASE ëª¨ë“œì—ì„œëŠ” 'ë°°ì†¡/ì„ ë¬¼/ê°€ê²©' ë“±ì€ ì‚´ë¦¼
    
    for res in tqdm(kiwi.analyze(texts), total=len(texts)):
        tokens = []
        try:
            if res and res[0] and res[0][0]:
                for token, pos, _, _ in res[0][0]:
                    
                    # 1. ë¸Œëœë“œ í•„í„° (ê³µí†µ)
                    if token.lower() in BRANDS:
                        continue
                        
                    # 2. ê¸°ë³¸ íŒ¨í„´ í™•ì¸ (ê¸¸ì´/í˜•ì‹)
                    # íš¨ëŠ¥ ëª¨ë“œë¼ë„ 'í•œê¸€ 1ê¸€ì'ëŠ” ë³´í†µ ë¬´ì˜ë¯¸ (í–¥, í†¤ ë“±ì€ ë¬¸ë§¥ ì—†ì´ ì¡ê¸° í˜ë“¦. í•„ìš”ì‹œ ì˜ˆì™¸ì²˜ë¦¬)
                    if not valid_pattern.fullmatch(token):
                         continue
                         
                    # 3. ë¶ˆìš©ì–´ í™•ì¸
                    if token in final_stopwords:
                        continue
                        
                    # 4. í’ˆì‚¬ë³„ ë¡œì§
                    if pos in ['NNG', 'NNP', 'VA', 'SL']:
                        # í˜•ìš©ì‚¬ ì›ë³µ
                        word = token + 'ë‹¤' if pos == 'VA' else token
                        
                        # [EFFICACY ëª¨ë“œ íŠ¹í™”] SL(ì˜ì–´) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì ìš©
                        if mode == 'EFFICACY' and pos == 'SL':
                            if word.lower() not in EFFICACY_SL_WHITELIST:
                                continue
                        
                        tokens.append(word)
        except Exception:
            pass
        
        tokenized_docs.append(" ".join(tokens))
        
    return tokenized_docs

def run_analysis(docs, mode='EFFICACY'):
    """
    ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë¡œì§
    """
    output_dir = f"{OUTPUT_DIR_ROOT}/{mode}"
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\n[{mode} Mode] Starting Analysis...")
    
    # 1. í† í°í™”
    print(f"  - Tokenizing...")
    pre_tokenized_docs = pre_tokenize(docs, mode=mode)
    
    # 2. ë¹ˆ ë¬¸ì„œ í•„í„°ë§
    print(f"  - Filtering empty documents...")
    valid_indices = [i for i, t in enumerate(pre_tokenized_docs) if t.strip()]
    
    filtered_docs = [docs[i] for i in valid_indices]
    filtered_tokens = [pre_tokenized_docs[i] for i in valid_indices]
    
    print(f"  - Removed {len(docs) - len(filtered_docs)} empty docs. Count: {len(filtered_docs)}")
    
    if len(filtered_docs) < 10:
        print("  ! Not enough data to proceed.")
        return

    # 3. ì„ë² ë”© (ìºì‹œ ê´€ë¦¬)
    # ëª¨ë“œë³„ë¡œ ì„ë² ë”©ì„ ê³µìœ í•  ìˆ˜ë„ ìˆì§€ë§Œ, filtered_docsê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ê´€ë¦¬ ê¶Œì¥ 
    # í˜¹ì€ 'filtered_docs' ë‚´ìš© ê¸°ë°˜ hashë¡œ ê´€ë¦¬
    
    emb_file = f"{output_dir}/embeddings.npy"
    hash_file = f"{output_dir}/embeddings_hash.txt"
    
    current_hash = compute_data_fingerprint(filtered_docs)
    embeddings = None
    embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    
    if os.path.exists(emb_file) and os.path.exists(hash_file):
        with open(hash_file, 'r') as f:
            saved_hash = f.read().strip()
        if saved_hash == current_hash:
            try:
                cached_emb = np.load(emb_file)
                if len(cached_emb) == len(filtered_docs):
                    embeddings = cached_emb
                    print("  - Cache Hit! Loaded embeddings.")
            except:
                pass
                
    if embeddings is None:
        print("  - Calculating embeddings...")
        embeddings = embedding_model.encode(
            filtered_docs, 
            show_progress_bar=True, 
            batch_size=64, 
            normalize_embeddings=True
        )
        np.save(emb_file, embeddings)
        with open(hash_file, 'w') as f:
            f.write(current_hash)
            
    # 4. BERTopic
    print("  - Fitting BERTopic...")
    # ëª¨ë“œë³„ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥
    min_topic_s = 30 if mode == 'EFFICACY' else 50
    
    vectorizer_model = CountVectorizer(
        tokenizer=None, preprocessor=None, analyzer='word',
        min_df=10, max_df=0.9, ngram_range=(1, 1), max_features=15000
    )
    
    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)
    
    topic_model = BERTopic(
        embedding_model=embedding_model,
        vectorizer_model=vectorizer_model,
        umap_model=umap_model,
        calculate_probabilities=False,
        min_topic_size=min_topic_s,
        verbose=True
    )
    
    topics, probs = topic_model.fit_transform(filtered_tokens, embeddings)
    
    # 5. ì €ì¥
    print(f"  - Saving results to {output_dir}...")
    topic_model.get_topic_info().to_csv(f"{output_dir}/topic_info.csv", index=False, encoding='utf-8-sig')
    
    # Doc Info Map
    doc_info = topic_model.get_document_info(filtered_tokens)
    doc_info['Review_Raw'] = filtered_docs
    doc_info = doc_info[['Topic', 'Review_Raw', 'Document', 'Representative_document']]
    doc_info.rename(columns={'Document':'Tokens'}, inplace=True)
    doc_info.to_csv(f"{output_dir}/document_topics.csv", index=False, encoding='utf-8-sig')
    
    # Rep Docs
    if 'Representative_document' in doc_info.columns:
        rep = doc_info[doc_info['Representative_document'] == True].copy()
        rep.to_csv(f"{output_dir}/topic_representative_reviews.csv", index=False, encoding='utf-8-sig')
        
    # Viz
    font_family = "Malgun Gothic"
    try:
        fig = topic_model.visualize_barchart(top_n_topics=15)
        fig.update_layout(font=dict(family=font_family))
        fig.write_html(f"{output_dir}/topics_barchart.html")
    except:
        pass

def main():
    print("\n[Step 1] Loading Data...")
    try:
        df = pd.read_csv(INPUT_FILE, encoding='utf-8-sig')
    except UnicodeDecodeError:
        df = pd.read_csv(INPUT_FILE, encoding='cp949')
        
    if 'Review' not in df.columns:
        return

    docs = df['Review'].dropna().tolist()
    docs = [str(doc) for doc in docs if len(str(doc)) > 5]
    print(f"Valid docs: {len(docs)}")
    
    # ---------------------------------------------------------
    # Dual Mode Execution
    # ---------------------------------------------------------
    # Mode 1: íš¨ëŠ¥/ì‚¬ìš©ê° ì¤‘ì‹¬ (EFFICACY)
    # - ë©”íƒ€ì–´(ë°°ì†¡/ì¶”ì²œ) ì œê±°
    # - ì˜ì–´(SL)ëŠ” Whitelist(ì„±ë¶„/ê¸°ëŠ¥)ë§Œ í—ˆìš©
    run_analysis(docs, mode='EFFICACY')
    
    # Mode 2: êµ¬ë§¤íŒ¨í„´/ë¼ì´í”„ìŠ¤íƒ€ì¼ ì¤‘ì‹¬ (PURCHASE)
    # - ë©”íƒ€ì–´(ì„ ë¬¼/ê°€ê²©/ì—„ë§ˆ ë“±) í—ˆìš©
    # - ì˜ì–´(SL) í—ˆìš©
    # - í† í”½ ì‚¬ì´ì¦ˆ ì¢€ ë” í¬ê²Œ ì¡ìŒ
    run_analysis(docs, mode='PURCHASE')
    
    print("\nâœ… All Dual-Mode Analyses Complete!")

if __name__ == "__main__":
    main()
